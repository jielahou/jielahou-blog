---
title: åœ¨æœ¬åœ°éƒ¨ç½²Llama-7Bæ¨¡å‹
---

> æˆ‘å¯¼ï¼šè·‘ä¸ªæ¨¡å‹å•Šï¼Ÿ
>
> æˆ‘ï¼šå¥½çš„è€å¸ˆï¼Œæ”¶åˆ°ã€‚

æœ¬æ–‡ä¸»è¦ä¾èµ–Huggingfaceæä¾›çš„`transformers`åº“è¿›è¡Œéƒ¨ç½²ã€æ¨ç†ã€‚å®˜æ–¹æ–‡æ¡£ï¼š[ğŸ¤— Transformers (huggingface.co)](https://huggingface.co/docs/transformers/v4.33.2/en/index)

## ä¸‹è½½æ¨¡å‹

> è®°å¾—æå‰é…ç½®å¥½ç½‘ç»œï¼Œç›´æ¥æ‹‰ï¼Œæ‹‰ä¸ä¸‹æ¥ã€‚2023.09.25

è¿™é‡Œç›´æ¥ä¸‹è½½Llama-7B `hf`æ ¼å¼çš„æ¨¡å‹ï¼šhttps://huggingface.co/decapoda-research/llama-7b-hf

> 1. `pip install huggingface_hub`
> 2. æ³¨æ„æ›´æ”¹`local_dir`å‚æ•°

```python
from huggingface_hub import snapshot_download

snapshot_download(repo_id="decapoda-research/llama-7b-hf", ignore_patterns=["*.h5", "*.ot", "*.msgpack"], local_dir="/data/llama_7b", local_dir_use_symlinks=False)
```

ç„¶åï¼Œä¿®æ”¹ä¸€ä¸‹`tokenizer_config.json`ï¼Œä¿®æ”¹å¦‚ä¸‹å‚æ•°ï¼ˆå¦åˆ™æŠ¥é”™ï¼Œå‚è€ƒï¼š[RecursionError: maximum recursion depth exceeded while getting the str of an object. Â· Issue #22762 Â· huggingface/transformers Â· GitHub](https://github.com/huggingface/transformers/issues/22762#issuecomment-1546774761)ï¼‰

```
unk_token="<unk>"
bos_token="<s>"
eos_token="</s>"
```





## é…ç½®æ¨ç†ç¯å¢ƒ

### å®‰è£…Transformersåº“

> æ³¨æ„æ˜¯`transformers`ï¼Œæ¯”`transformer`å¤šäº†ä¸€ä¸ª`s`

å‚è€ƒï¼š[Installation (huggingface.co)](https://huggingface.co/docs/transformers/v4.18.0/en/installation#cache-setup)

æ³¨æ„ï¼šæ–‡æ¡£ä¸­`Cache setup`å¾ˆé‡è¦ï¼Œå†³å®šäº†åç»­ç”¨Transformersåº“æ—¶ï¼Œç¼“å­˜çš„èµ„æºä¿å­˜åœ¨ä½•å¤„ã€‚ä¾‹å¦‚ï¼šæˆ‘ä»¬æœåŠ¡å™¨ä¸Šå•ç‹¬æŒ‚äº†ä¸€ä¸ª14Tçš„ç›˜åˆ°`/data`ï¼Œé‚£å°±è®¾ç½®ä¸€ä¸ªç¯å¢ƒå˜é‡ï¼ˆä¿®æ”¹å¯¹åº”shellçš„`~/.æŸshrc`ï¼Œå¹¶`source`ä¸€ä¸‹ï¼Œä»¥åº”ç”¨åˆ°å½“å‰ä¼šè¯ï¼‰

```shell
export TRANSFORMERS_CACHE=/data/XXX/transformers_cache
```

### å®‰è£…optimumåº“

æ¨ç†è¦ç”¨åˆ°æ­¤åº“ã€‚

```shell
pip install optimum
```

### ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†

å‚è€ƒï¼š[Pipelines for inference (huggingface.co)](https://huggingface.co/docs/transformers/v4.33.2/en/pipeline_tutorial)

å‚è€ƒï¼š[Efficient Inference on a Multiple GPUs (huggingface.co)](https://huggingface.co/docs/transformers/perf_infer_gpu_many)

å‚è€ƒï¼š[LLaMA (huggingface.co)](https://huggingface.co/docs/transformers/main/en/model_doc/llama)

> çªç„¶å‘ç°Huggingfaceæœ‰ä¸ªåŠŸèƒ½æŒºä¸é”™ï¼Œè¦æ˜¯åŸºäºTransformersæ¨¡å—åšæ¨ç†ï¼Œå¯ä»¥ç‚¹å‡»å³ä¸Šè§’`User in Transformers`ç”ŸæˆåŸºæœ¬ä»£ç ã€‚

> ![è‡ªåŠ¨ç”Ÿæˆåˆå§‹åŒ–ä»£ç ](./run-llama-7b-inference.assets/æˆªå›¾_20230925102229.png)

ç„¶åè‡ªå·±å†™ä¸€ä¸‹æ¨ç†è„šæœ¬ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
def Ask(text):
    input_text = text;
    inputs = tokenizer(input_text, return_tensors="pt").to("cuda")
    # with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    generate_ids = model.generate(inputs.input_ids, max_length=100)
    print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])

tokenizer = AutoTokenizer.from_pretrained("ä¸‹ä¸‹æ¥æ¨¡å‹çš„ä½ç½®")
### ERROR - transformers.tokenization_utils -   Using pad_token, but it is not set yet
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained("ä¸‹ä¸‹æ¥æ¨¡å‹çš„ä½ç½®").to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

```

## è¿›è¡Œæ¨ç†ï¼

![è¿è¡Œç»“æœ](./run-llama-7b-inference.assets/img_v2_0723b7e0-9ccd-477a-9e42-e2a8b5a55a6g.png)

æœ‰ç‚¹æ†¨æ†¨çš„ã€‚
