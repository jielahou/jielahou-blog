import{_ as o,F as c,W as p,X as r,Y as n,Z as s,a0 as e,$ as t}from"./framework-dfe78a98.js";const i="/assets/截图_20230925102229-1ea4779b.png",l="/assets/img_v2_0723b7e0-9ccd-477a-9e42-e2a8b5a55a6g-9e7a8e9f.png",u={},d=n("blockquote",null,[n("p",null,"我导：跑个模型啊？"),n("p",null,"我：好的老师，收到。")],-1),k=n("code",null,"transformers",-1),h={href:"https://huggingface.co/docs/transformers/v4.33.2/en/index",target:"_blank",rel:"noopener noreferrer"},m=n("h2",{id:"下载模型",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#下载模型","aria-hidden":"true"},"#"),s(" 下载模型")],-1),g=n("blockquote",null,[n("p",null,"记得提前配置好网络，直接拉，拉不下来。2023.09.25")],-1),_=n("code",null,"hf",-1),f={href:"https://huggingface.co/decapoda-research/llama-7b-hf",target:"_blank",rel:"noopener noreferrer"},b=t(`<blockquote><ol><li><code>pip install huggingface_hub</code></li><li>注意更改<code>local_dir</code>参数</li></ol></blockquote><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> huggingface_hub <span class="token keyword">import</span> snapshot_download

snapshot_download<span class="token punctuation">(</span>repo_id<span class="token operator">=</span><span class="token string">&quot;decapoda-research/llama-7b-hf&quot;</span><span class="token punctuation">,</span> ignore_patterns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;*.h5&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;*.ot&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;*.msgpack&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> local_dir<span class="token operator">=</span><span class="token string">&quot;/data/llama_7b&quot;</span><span class="token punctuation">,</span> local_dir_use_symlinks<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2),v=n("code",null,"tokenizer_config.json",-1),q={href:"https://github.com/huggingface/transformers/issues/22762#issuecomment-1546774761",target:"_blank",rel:"noopener noreferrer"},x=t(`<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>unk_token=&quot;&lt;unk&gt;&quot;
bos_token=&quot;&lt;s&gt;&quot;
eos_token=&quot;&lt;/s&gt;&quot;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="配置推理环境" tabindex="-1"><a class="header-anchor" href="#配置推理环境" aria-hidden="true">#</a> 配置推理环境</h2><h3 id="安装transformers库" tabindex="-1"><a class="header-anchor" href="#安装transformers库" aria-hidden="true">#</a> 安装Transformers库</h3><blockquote><p>注意是<code>transformers</code>，比<code>transformer</code>多了一个<code>s</code></p></blockquote>`,4),y={href:"https://huggingface.co/docs/transformers/v4.18.0/en/installation#cache-setup",target:"_blank",rel:"noopener noreferrer"},T=t(`<p>注意：文档中<code>Cache setup</code>很重要，决定了后续用Transformers库时，缓存的资源保存在何处。例如：我们服务器上单独挂了一个14T的盘到<code>/data</code>，那就设置一个环境变量（修改对应shell的<code>~/.某shrc</code>，并<code>source</code>一下，以应用到当前会话）</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">TRANSFORMERS_CACHE</span><span class="token operator">=</span>/data/XXX/transformers_cache
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="安装optimum库" tabindex="-1"><a class="header-anchor" href="#安装optimum库" aria-hidden="true">#</a> 安装optimum库</h3><p>推理要用到此库。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> optimum
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="使用pipeline进行推理" tabindex="-1"><a class="header-anchor" href="#使用pipeline进行推理" aria-hidden="true">#</a> 使用pipeline进行推理</h3>`,6),w={href:"https://huggingface.co/docs/transformers/v4.33.2/en/pipeline_tutorial",target:"_blank",rel:"noopener noreferrer"},z={href:"https://huggingface.co/docs/transformers/perf_infer_gpu_many",target:"_blank",rel:"noopener noreferrer"},A={href:"https://huggingface.co/docs/transformers/main/en/model_doc/llama",target:"_blank",rel:"noopener noreferrer"},E=t('<blockquote><p>突然发现Huggingface有个功能挺不错，要是基于Transformers模块做推理，可以点击右上角<code>User in Transformers</code>生成基本代码。</p></blockquote><blockquote><figure><img src="'+i+`" alt="自动生成初始化代码" tabindex="0"><figcaption>自动生成初始化代码</figcaption></figure></blockquote><p>然后自己写一下推理脚本：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer
<span class="token keyword">def</span> <span class="token function">Ask</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_text <span class="token operator">=</span> text<span class="token punctuation">;</span>
    inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>input_text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span>
    <span class="token comment"># with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):</span>
    generate_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generate_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;下下来模型的位置&quot;</span><span class="token punctuation">)</span>
<span class="token comment">### ERROR - transformers.tokenization_utils -   Using pad_token, but it is not set yet</span>
tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;下下来模型的位置&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span>
<span class="token comment"># convert the model to BetterTransformer</span>
model<span class="token punctuation">.</span>to_bettertransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="进行推理" tabindex="-1"><a class="header-anchor" href="#进行推理" aria-hidden="true">#</a> 进行推理！</h2><figure><img src="`+l+'" alt="运行结果" tabindex="0"><figcaption>运行结果</figcaption></figure><p>有点憨憨的。</p>',7);function F(L,M){const a=c("ExternalLinkIcon");return p(),r("div",null,[d,n("p",null,[s("本文主要依赖Huggingface提供的"),k,s("库进行部署、推理。官方文档："),n("a",h,[s("🤗 Transformers (huggingface.co)"),e(a)])]),m,g,n("p",null,[s("这里直接下载Llama-7B "),_,s("格式的模型："),n("a",f,[s("https://huggingface.co/decapoda-research/llama-7b-hf"),e(a)])]),b,n("p",null,[s("然后，修改一下"),v,s("，修改如下参数（否则报错，参考："),n("a",q,[s("RecursionError: maximum recursion depth exceeded while getting the str of an object. · Issue #22762 · huggingface/transformers · GitHub"),e(a)]),s("）")]),x,n("p",null,[s("参考："),n("a",y,[s("Installation (huggingface.co)"),e(a)])]),T,n("p",null,[s("参考："),n("a",w,[s("Pipelines for inference (huggingface.co)"),e(a)])]),n("p",null,[s("参考："),n("a",z,[s("Efficient Inference on a Multiple GPUs (huggingface.co)"),e(a)])]),n("p",null,[s("参考："),n("a",A,[s("LLaMA (huggingface.co)"),e(a)])]),E])}const C=o(u,[["render",F],["__file","run-llama-7b-inference.html.vue"]]);export{C as default};
