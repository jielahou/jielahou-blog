import{_ as o,F as c,W as p,X as r,Y as n,Z as s,a0 as e,$ as t}from"./framework-dfe78a98.js";const i="/assets/æˆªå›¾_20230925102229-1ea4779b.png",l="/assets/img_v2_0723b7e0-9ccd-477a-9e42-e2a8b5a55a6g-9e7a8e9f.png",u={},d=n("blockquote",null,[n("p",null,"æˆ‘å¯¼ï¼šè·‘ä¸ªæ¨¡å‹å•Šï¼Ÿ"),n("p",null,"æˆ‘ï¼šå¥½çš„è€å¸ˆï¼Œæ”¶åˆ°ã€‚")],-1),k=n("code",null,"transformers",-1),h={href:"https://huggingface.co/docs/transformers/v4.33.2/en/index",target:"_blank",rel:"noopener noreferrer"},m=n("h2",{id:"ä¸‹è½½æ¨¡å‹",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#ä¸‹è½½æ¨¡å‹","aria-hidden":"true"},"#"),s(" ä¸‹è½½æ¨¡å‹")],-1),g=n("blockquote",null,[n("p",null,"è®°å¾—æå‰é…ç½®å¥½ç½‘ç»œï¼Œç›´æ¥æ‹‰ï¼Œæ‹‰ä¸ä¸‹æ¥ã€‚2023.09.25")],-1),_=n("code",null,"hf",-1),f={href:"https://huggingface.co/decapoda-research/llama-7b-hf",target:"_blank",rel:"noopener noreferrer"},b=t(`<blockquote><ol><li><code>pip install huggingface_hub</code></li><li>æ³¨æ„æ›´æ”¹<code>local_dir</code>å‚æ•°</li></ol></blockquote><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> huggingface_hub <span class="token keyword">import</span> snapshot_download

snapshot_download<span class="token punctuation">(</span>repo_id<span class="token operator">=</span><span class="token string">&quot;decapoda-research/llama-7b-hf&quot;</span><span class="token punctuation">,</span> ignore_patterns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;*.h5&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;*.ot&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;*.msgpack&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> local_dir<span class="token operator">=</span><span class="token string">&quot;/data/llama_7b&quot;</span><span class="token punctuation">,</span> local_dir_use_symlinks<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2),v=n("code",null,"tokenizer_config.json",-1),q={href:"https://github.com/huggingface/transformers/issues/22762#issuecomment-1546774761",target:"_blank",rel:"noopener noreferrer"},x=t(`<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>unk_token=&quot;&lt;unk&gt;&quot;
bos_token=&quot;&lt;s&gt;&quot;
eos_token=&quot;&lt;/s&gt;&quot;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="é…ç½®æ¨ç†ç¯å¢ƒ" tabindex="-1"><a class="header-anchor" href="#é…ç½®æ¨ç†ç¯å¢ƒ" aria-hidden="true">#</a> é…ç½®æ¨ç†ç¯å¢ƒ</h2><h3 id="å®‰è£…transformersåº“" tabindex="-1"><a class="header-anchor" href="#å®‰è£…transformersåº“" aria-hidden="true">#</a> å®‰è£…Transformersåº“</h3><blockquote><p>æ³¨æ„æ˜¯<code>transformers</code>ï¼Œæ¯”<code>transformer</code>å¤šäº†ä¸€ä¸ª<code>s</code></p></blockquote>`,4),y={href:"https://huggingface.co/docs/transformers/v4.18.0/en/installation#cache-setup",target:"_blank",rel:"noopener noreferrer"},T=t(`<p>æ³¨æ„ï¼šæ–‡æ¡£ä¸­<code>Cache setup</code>å¾ˆé‡è¦ï¼Œå†³å®šäº†åç»­ç”¨Transformersåº“æ—¶ï¼Œç¼“å­˜çš„èµ„æºä¿å­˜åœ¨ä½•å¤„ã€‚ä¾‹å¦‚ï¼šæˆ‘ä»¬æœåŠ¡å™¨ä¸Šå•ç‹¬æŒ‚äº†ä¸€ä¸ª14Tçš„ç›˜åˆ°<code>/data</code>ï¼Œé‚£å°±è®¾ç½®ä¸€ä¸ªç¯å¢ƒå˜é‡ï¼ˆä¿®æ”¹å¯¹åº”shellçš„<code>~/.æŸshrc</code>ï¼Œå¹¶<code>source</code>ä¸€ä¸‹ï¼Œä»¥åº”ç”¨åˆ°å½“å‰ä¼šè¯ï¼‰</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">TRANSFORMERS_CACHE</span><span class="token operator">=</span>/data/XXX/transformers_cache
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="å®‰è£…optimumåº“" tabindex="-1"><a class="header-anchor" href="#å®‰è£…optimumåº“" aria-hidden="true">#</a> å®‰è£…optimumåº“</h3><p>æ¨ç†è¦ç”¨åˆ°æ­¤åº“ã€‚</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> optimum
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†" tabindex="-1"><a class="header-anchor" href="#ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†" aria-hidden="true">#</a> ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†</h3>`,6),w={href:"https://huggingface.co/docs/transformers/v4.33.2/en/pipeline_tutorial",target:"_blank",rel:"noopener noreferrer"},z={href:"https://huggingface.co/docs/transformers/perf_infer_gpu_many",target:"_blank",rel:"noopener noreferrer"},A={href:"https://huggingface.co/docs/transformers/main/en/model_doc/llama",target:"_blank",rel:"noopener noreferrer"},E=t('<blockquote><p>çªç„¶å‘ç°Huggingfaceæœ‰ä¸ªåŠŸèƒ½æŒºä¸é”™ï¼Œè¦æ˜¯åŸºäºTransformersæ¨¡å—åšæ¨ç†ï¼Œå¯ä»¥ç‚¹å‡»å³ä¸Šè§’<code>User in Transformers</code>ç”ŸæˆåŸºæœ¬ä»£ç ã€‚</p></blockquote><blockquote><figure><img src="'+i+`" alt="è‡ªåŠ¨ç”Ÿæˆåˆå§‹åŒ–ä»£ç " tabindex="0"><figcaption>è‡ªåŠ¨ç”Ÿæˆåˆå§‹åŒ–ä»£ç </figcaption></figure></blockquote><p>ç„¶åè‡ªå·±å†™ä¸€ä¸‹æ¨ç†è„šæœ¬ï¼š</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer
<span class="token keyword">def</span> <span class="token function">Ask</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_text <span class="token operator">=</span> text<span class="token punctuation">;</span>
    inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>input_text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span>
    <span class="token comment"># with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):</span>
    generate_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generate_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;ä¸‹ä¸‹æ¥æ¨¡å‹çš„ä½ç½®&quot;</span><span class="token punctuation">)</span>
<span class="token comment">### ERROR - transformers.tokenization_utils -   Using pad_token, but it is not set yet</span>
tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;ä¸‹ä¸‹æ¥æ¨¡å‹çš„ä½ç½®&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">)</span>
<span class="token comment"># convert the model to BetterTransformer</span>
model<span class="token punctuation">.</span>to_bettertransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="è¿›è¡Œæ¨ç†" tabindex="-1"><a class="header-anchor" href="#è¿›è¡Œæ¨ç†" aria-hidden="true">#</a> è¿›è¡Œæ¨ç†ï¼</h2><figure><img src="`+l+'" alt="è¿è¡Œç»“æœ" tabindex="0"><figcaption>è¿è¡Œç»“æœ</figcaption></figure><p>æœ‰ç‚¹æ†¨æ†¨çš„ã€‚</p>',7);function F(L,M){const a=c("ExternalLinkIcon");return p(),r("div",null,[d,n("p",null,[s("æœ¬æ–‡ä¸»è¦ä¾èµ–Huggingfaceæä¾›çš„"),k,s("åº“è¿›è¡Œéƒ¨ç½²ã€æ¨ç†ã€‚å®˜æ–¹æ–‡æ¡£ï¼š"),n("a",h,[s("ğŸ¤— Transformers (huggingface.co)"),e(a)])]),m,g,n("p",null,[s("è¿™é‡Œç›´æ¥ä¸‹è½½Llama-7B "),_,s("æ ¼å¼çš„æ¨¡å‹ï¼š"),n("a",f,[s("https://huggingface.co/decapoda-research/llama-7b-hf"),e(a)])]),b,n("p",null,[s("ç„¶åï¼Œä¿®æ”¹ä¸€ä¸‹"),v,s("ï¼Œä¿®æ”¹å¦‚ä¸‹å‚æ•°ï¼ˆå¦åˆ™æŠ¥é”™ï¼Œå‚è€ƒï¼š"),n("a",q,[s("RecursionError: maximum recursion depth exceeded while getting the str of an object. Â· Issue #22762 Â· huggingface/transformers Â· GitHub"),e(a)]),s("ï¼‰")]),x,n("p",null,[s("å‚è€ƒï¼š"),n("a",y,[s("Installation (huggingface.co)"),e(a)])]),T,n("p",null,[s("å‚è€ƒï¼š"),n("a",w,[s("Pipelines for inference (huggingface.co)"),e(a)])]),n("p",null,[s("å‚è€ƒï¼š"),n("a",z,[s("Efficient Inference on a Multiple GPUs (huggingface.co)"),e(a)])]),n("p",null,[s("å‚è€ƒï¼š"),n("a",A,[s("LLaMA (huggingface.co)"),e(a)])]),E])}const C=o(u,[["render",F],["__file","run-llama-7b-inference.html.vue"]]);export{C as default};
