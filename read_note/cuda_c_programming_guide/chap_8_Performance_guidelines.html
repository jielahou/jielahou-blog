<!DOCTYPE html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.60" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://mister-hope.github.io/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html"><meta property="og:site_name" content="Jielahou's Blog"><meta property="og:title" content="《CUDA C Programming》第八章 性能准则"><meta property="og:description" content="本章原文：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines 内容有些杂、有些乱，主要是自己学习用的，请谅解！"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2024-01-10T06:50:01.000Z"><meta property="article:modified_time" content="2024-01-10T06:50:01.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"《CUDA C Programming》第八章 性能准则","image":[""],"dateModified":"2024-01-10T06:50:01.000Z","author":[]}</script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js?id=KDeTDcfevxjOrgMS&ck=KDeTDcfevxjOrgMS"></script><title>《CUDA C Programming》第八章 性能准则 | Jielahou's Blog</title><meta name="description" content="本章原文：https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines 内容有些杂、有些乱，主要是自己学习用的，请谅解！">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-122b92f0.css" as="style"><link rel="stylesheet" href="/assets/style-122b92f0.css">
    <link rel="modulepreload" href="/assets/app-ce01a4a7.js"><link rel="modulepreload" href="/assets/framework-532589df.js"><link rel="modulepreload" href="/assets/chap_8_Performance_guidelines.html-7370159d.js"><link rel="modulepreload" href="/assets/chap_8_Performance_guidelines.html-bb84a51d.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><a href="/" class="brand"><!----><!----><span class="site-name">Jielahou&#39;s Blog</span></a><!--[--><!----><!--]--></div><div class="navbar-center"><!--[--><!----><!--]--><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/" class="nav-link" aria-label="主页"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><a href="/intro.html" class="nav-link" aria-label="关于"><span class="font-icon icon iconfont icon-info" style=""></span>关于<!----></a></div><div class="nav-item hide-in-mobile"><a href="/link.html" class="nav-link" aria-label="友链"><span class="font-icon icon iconfont icon-link" style=""></span>友链<!----></a></div></nav><!--[--><!----><!--]--></div><div class="navbar-right"><!--[--><!----><!--]--><!----><div class="nav-item"><a class="repo-link" href="https://github.com/jielahou/jielahou-blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!--[--><!----><!--]--><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->《CUDA C Programming》第八章 性能准则</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://jielahou.com" target="_blank" rel="noopener noreferrer">jielahou</a></span><span property="author" content="jielahou"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-01-04T10:50:02.000Z"></span><!----><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 23 分钟</span><meta property="timeRequired" content="PT23M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">此页内容</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#优化策略概览" class="router-link-active router-link-exact-active toc-link level2">优化策略概览</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#最大化利用率" class="router-link-active router-link-exact-active toc-link level2">最大化利用率</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#应用层面" class="router-link-active router-link-exact-active toc-link level3">应用层面</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#设备层面" class="router-link-active router-link-exact-active toc-link level3">设备层面</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#sm层面" class="router-link-active router-link-exact-active toc-link level3">SM层面</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#最大化内存吞吐量" class="router-link-active router-link-exact-active toc-link level2">最大化内存吞吐量</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#主机和设备之间的数据传输" class="router-link-active router-link-exact-active toc-link level3">主机和设备之间的数据传输</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#设备内存访问" class="router-link-active router-link-exact-active toc-link level3">设备内存访问</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.html#最大化指令吞吐量" class="router-link-active router-link-exact-active toc-link level2">最大化指令吞吐量</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><blockquote><p>本章原文：<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines" target="_blank" rel="noopener noreferrer">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>内容有些杂、有些乱，主要是自己学习用的，请谅解！</p></blockquote><h2 id="优化策略概览" tabindex="-1"><a class="header-anchor" href="#优化策略概览" aria-hidden="true">#</a> 优化策略概览</h2><blockquote><p>Performance optimization revolves around four basic strategies:</p><ul><li>Maximize parallel execution to achieve maximum utilization;</li><li>Optimize memory usage to achieve maximum memory throughput;</li><li>Optimize instruction usage to achieve maximum instruction throughput;</li><li>Minimize memory thrashing.</li></ul></blockquote><p>性能优化围绕四个基本策略展开：</p><ul><li><p><strong>最大化并行执行</strong>，实现最大利用率；</p></li><li><p><strong>优化内存使用</strong>，实现最大内存吞吐量；</p></li><li><p><strong>优化指令使用</strong>，实现最大指令吞吐量；</p></li><li><p><strong>尽量减少内存抖动</strong></p></li></ul><div class="hint-container info"><p class="hint-container-title">内存抖动</p><p>内存抖动应该是指内存页面频繁的换入换出</p></div><blockquote><p>Which strategies will yield the best performance gain for a particular portion of an application depends on the performance limiters for that portion; optimizing instruction usage of a kernel that is mostly limited by memory accesses will not yield any significant performance gain, for example. Optimization efforts should therefore be constantly directed by measuring and monitoring the performance limiters, for example using the CUDA profiler. Also, comparing the floating-point operation throughput or memory throughput—whichever makes more sense—of a particular kernel to the corresponding peak theoretical throughput of the device indicates how much room for improvement there is for the kernel.</p></blockquote><p>对于应用程序的特定部分， 哪种优化策略能取得最大的性能提升， 取决于<strong>到底是哪部分限制住了性能</strong>；例如， 对受内存访问限制的内核进行指令上的优化， 不会带来任何显著的性能提升。 因此， 应<strong>通过测量和监控</strong>（例如使用 CUDA profiler），去<strong>发现性能限制因素</strong>，再来不断指导优化工作。 此外， 将特定内核的浮点运算吞吐量或内存吞吐量（以更合理的方式为准） 与相应设备的理论峰值吞吐量进行比较，可以显示内核的改进空间有多大。</p><h2 id="最大化利用率" tabindex="-1"><a class="header-anchor" href="#最大化利用率" aria-hidden="true">#</a> 最大化利用率</h2><blockquote><p>To maximize utilization the application should be structured in a way that it exposes as much parallelism as possible and efficiently maps this parallelism to the various components of the system to keep them busy most of the time.</p></blockquote><p>为了最大限度地提高利用率， 应用程序的结构应尽可能多地<strong>暴露出并行性</strong>， 并有效地<strong>将这种并行性映射到系统的各个组件上</strong>，使它们在大部分时间内都处于忙碌状态。</p><h3 id="应用层面" tabindex="-1"><a class="header-anchor" href="#应用层面" aria-hidden="true">#</a> 应用层面</h3><blockquote><p>At a high level, the application should maximize parallel execution between the host, the devices, and the bus connecting the host to the devices, by <strong>using asynchronous functions calls and streams</strong> as described in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution" target="_blank" rel="noopener noreferrer">Asynchronous Concurrent Execution<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. It should assign to each processor the type of work it does best: serial workloads to the host; parallel workloads to the devices.</p></blockquote><p>在高层次上， 应用程序应通过使用==<strong>异步函数调用和流机制</strong>==（如异步并发执行中所述），最大限度地提高主机、 设备以及连接主机和设备的总线之间的并行执行能力。 它应为每个处理器分配其最擅长的工作类型：串行工作负载分配给主机；并行工作负载分配给设备。</p><blockquote><p>For the parallel workloads, at points in the algorithm where parallelism is broken because some threads need to synchronize in order to share data with each other, there are two cases: Either these threads belong to the same block, in which case they should use <code>__syncthreads()</code> and share data through shared memory within the same kernel invocation, or they belong to different blocks, in which case they must share data through global memory using two separate kernel invocations, one for writing to and one for reading from global memory. The second case is much less optimal since it adds the overhead of extra kernel invocations and global memory traffic. Its occurrence should therefore be minimized by mapping the algorithm to the CUDA programming model in such a way that the computations that require inter-thread communication are performed within a single thread block as much as possible.</p></blockquote><p>对于并行工作负载，在算法中由于<strong>某些线程需要同步以相互共享数据而导致并行性中断</strong>的地方，<strong>有两种情况</strong>：<strong>要么这些线程属于同一个Block</strong>，在这种情况下，它们应该使用<code>syncthreads()</code>并在同一个内核调用中<strong>通过共享存储器</strong>共享数据；<strong>要么它们属于不同的区块</strong>，在这种情况下，它们必须<strong>调用两个单独的内核通过全局存储器</strong>共享数据，一个用于写入全局内存，另一个用于从全局内存读取数据。第二种情况并不理想，因为它会增加额外的内核调用和全局存储器流量。因此，在将算法映射到CUDA编程模型时，线程间若需要需要通信，应尽可能<strong>在单个线程块内执行计算</strong>，从而最大限度地减少这种情况的发生。</p><div class="hint-container info"><p class="hint-container-title">相关信息</p><p>总结：</p><ul><li>异步函数调用和流机制</li><li>当线程间需要通信时，尽量在单个线程块内执行</li></ul></div><h3 id="设备层面" tabindex="-1"><a class="header-anchor" href="#设备层面" aria-hidden="true">#</a> 设备层面</h3><blockquote><p>At a lower level, the application should maximize parallel execution between the multiprocessors of a device.</p></blockquote><p>在较低层次上，应用程序应最大限度地在设备的多处理器之间并行执行。</p><blockquote><p>Multiple kernels can execute concurrently on a device, so maximum utilization can also be achieved by using streams to enable enough kernels to execute concurrently as described in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution" target="_blank" rel="noopener noreferrer">Asynchronous Concurrent Execution<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p></blockquote><p>一个设备上可同时执行多个内核，因此，如异步并发执行中所述，使用流机制使足够多的内核同时执行，也可实现最大利用率。</p><div class="hint-container info"><p class="hint-container-title">相关信息</p><p>总结：使用流机制让内核并行</p></div><h3 id="sm层面" tabindex="-1"><a class="header-anchor" href="#sm层面" aria-hidden="true">#</a> SM层面</h3><blockquote><p>At an even lower level, the application should <strong>maximize parallel execution between the various functional units</strong> within a multiprocessor.</p></blockquote><p>在更低的层次上，应用程序应最大限度地利用<strong>多处理器内各功能单元之间的并行执行</strong>。</p><blockquote><p>As described in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading" target="_blank" rel="noopener noreferrer">Hardware Multithreading<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, a GPU multiprocessor primarily relies on thread-level parallelism to maximize utilization of its functional units. Utilization is therefore directly linked to the number of resident warps. At every instruction issue time, a warp scheduler selects an instruction that is ready to execute. This instruction can be another independent instruction of the same warp, exploiting instruction-level parallelism, or more commonly an instruction of another warp, exploiting thread-level parallelism. If a ready to execute instruction is selected it is issued to the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture-notes" target="_blank" rel="noopener noreferrer">active<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> threads of the warp. The number of clock cycles it takes for a warp to be ready to execute its next instruction is called the <em>latency</em>, and full utilization is achieved when all warp schedulers always have some instruction to issue for some warp at every clock cycle during that latency period, or in other words, when latency is completely “hidden”. The number of instructions required to hide a latency of L clock cycles depends on the respective throughputs of these instructions (see <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions" target="_blank" rel="noopener noreferrer">Arithmetic Instructions<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> for the throughputs of various arithmetic instructions). If we assume instructions with maximum throughput, it is equal to:</p></blockquote><p>如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading" target="_blank" rel="noopener noreferrer">硬件多线程<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>所述，GPU 多处理器<strong>主要依靠线程级并行</strong>来最大限度地利用其功能单元。因此，利用率与常驻的线程束数量直接相关。在每条指令发出时，线程束调度器都会选择一条准备执行的指令。这条指令可以是<strong>利用同一 warp 的另一条独立指令（指令级并行性）</strong>，也可以是<strong>利用另一个 warp 的指令（线程级并行性）</strong>。如果选择了一条准备执行的指令，它就会被发射给该 warp 的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture-notes" target="_blank" rel="noopener noreferrer">活跃<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>线程。<strong>线程束准备好执行下一条指令所需的时钟周期数称为<em>延迟</em></strong>，当所有线程束调度器在该延迟期内的每个时钟周期都要为一些线程束发射指令时，或者换句话说，当<strong>延迟被完全 &quot;隐藏&quot;<strong>时，就实现了充分利用。隐藏 L 个时钟周期的延迟所需的指令数取决于这些指令各自的吞吐量（各种算术指令的吞吐量参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions" target="_blank" rel="noopener noreferrer">算术指令<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）。如果假定指令的</strong>吞吐量</strong>最大，则等于：</p><div class="hint-container info"><p class="hint-container-title">吞吐量</p><p>关于吞吐量定义可见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#maximize-instruction-throughput" target="_blank" rel="noopener noreferrer">这里<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。<strong><mark>吞吐量</mark><strong>是指 对于某指定操作 每个SM、每个时钟周期</strong>能够执行该操作的次数</strong>。对于32个线程组成的线程束，一条指令会对应32次操作；所以如果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>是每个时钟周期执行操作的次数，那么**<mark>指令吞吐量</mark>就是每个周期<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mi mathvariant="normal">/</mi><mn>32</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N/32)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord">/32</span><span class="mclose">)</span></span></span></span>条<mark>指令</mark>**。</p><p>所有的吞吐量都是针对一个SM而言的。在计算整个设备的吞吐量时，需要将吞吐量乘上该设备拥有的SM个数。</p><p>至于上文提到的“假定指令吞吐量最大”，个人认为指的是 能够打满相关的执行单元。</p></div><ul><li><em>4L</em> for devices of compute capability 5.x, 6.1, 6.2, 7.x and 8.x since for these devices, a multiprocessor issues one instruction per warp over one clock cycle for four warps at a time, as mentioned in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities" target="_blank" rel="noopener noreferrer">Compute Capabilities<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. <ul><li>一个SM 在一个时钟周期 为4个线程束中每个线程束 发射一条指令，共发射4条指令</li><li>即：一个SM 在一个时钟周期 会发射4条指令</li></ul></li><li><em>2L</em> for devices of compute capability 6.0 since for these devices, the two instructions issued every cycle are one instruction for two different warps. <ul><li>对于计算能力6.0的设备</li><li>一个周期发射两条指令，这两条指令来自不同的线程束。</li></ul></li></ul><blockquote><p>The most common reason a warp is not ready to execute its next instruction is that the instruction’s input operands are not available yet.</p></blockquote><p>线程束之所以尚未准备好执行下一条指令，最常见的原因是指令的输入操作数还没准备好。</p><p><mark>还有很多，鸽了</mark></p><h2 id="最大化内存吞吐量" tabindex="-1"><a class="header-anchor" href="#最大化内存吞吐量" aria-hidden="true">#</a> 最大化内存吞吐量</h2><p>减少低带宽的数据传输，即减少主机和设备之间的数据传输、即减少全局存储器和设备之间的数据传输（=&gt;善用<strong>共享存储器</strong>和各种Cache）。</p><p><strong>共享存储器</strong>相当于<strong>用户管理的高速缓存</strong>： 应用程序<strong>明确分配和访问</strong>它。</p><p>片上内存既用于 L1 ，也用于共享内存；片上内存有多少分给 L1 、多少分给共享内存，在每次内核调用时可以进行配置。</p><p>内核访问内存的吞吐量会因每种内存的访问模式不同而有数量级的差异。因此，最大化内存吞吐量的下一步就是根据<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses" target="_blank" rel="noopener noreferrer">设备内存访问<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>中描述的<strong>最佳内存访问模式</strong>，尽可能优化内存访问。这种优化对全局内存访问尤为重要，因为与可用的片上带宽和算术指令吞吐量相比，全局内存带宽是较低的，因此非最佳全局内存访问通常会对性能产生很大影响。</p><div class="hint-container info"><p class="hint-container-title">小结</p><ol><li>使用共享存储器</li><li>使用最佳内存访问模式</li></ol></div><h3 id="主机和设备之间的数据传输" tabindex="-1"><a class="header-anchor" href="#主机和设备之间的数据传输" aria-hidden="true">#</a> 主机和设备之间的数据传输</h3><p>若要减少主机和设备之间的数据传输，（如果有些工作主机和设备都能做，那么）可以让设备执行更多的代码（而不是在主机上执行，虽然这可能会降低设备代码的并行性）。如此一来，<strong><mark>中间数据结构可在设备内存中创建</mark></strong>，由设备操作并销毁，而无需由主机映射或复制到主机内存中。</p><p>此外，由于<strong>每次传输都会产生开销</strong>，因此将**<mark>许多小传输批量合并为一次大传输</mark>**，总比单独进行每次传输要好。</p><p>使用**<mark>锁页内存</mark>**（Page-Locked Host Memory）（使用<code>cudaHostAlloc</code>、<code>cudaHostFree</code>分配/销毁的就是锁页内存），好处（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#page-locked-host-memory" target="_blank" rel="noopener noreferrer">Page-Locked Host Memory<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）：</p><ul><li>Copies <strong>between page-locked host memory and device memory can be performed concurrently</strong> with kernel execution for some devices as mentioned in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution" target="_blank" rel="noopener noreferrer">Asynchronous Concurrent Execution<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. <ul><li><mark>？？复制并行执行？？</mark></li></ul></li><li>On some devices, <strong>page-locked host memory can be mapped into the address space of the device</strong>, eliminating the need to copy it to or from device memory as detailed in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#mapped-memory" target="_blank" rel="noopener noreferrer">Mapped Memory<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. <ul><li><strong>主机上的锁页内存可以映射到设备的地址空间中</strong>，进而<strong>设备可以直接访问主机内存</strong>，无需（显式地，存疑）先把数据从主机内存搬到全局存储器再访问。这既是“映射内存”（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#mapped-memory" target="_blank" rel="noopener noreferrer">Mapped Memory<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>），也是**所谓的“零拷贝”（Zero-Copy）**内存。</li><li>如果主存中的数据只被访问1次，那么直接从主存中映射访问要比开一块全局存储器，从主存搬运到全局存储器要快。</li><li><strong>准确来说，零拷贝并不是无需拷贝，而是无需显式拷贝</strong>。使用零拷贝内存时不需要cudaMemcpy之类的显式拷贝操作，直接通过指针取值，所以对调用者来说似乎是没有拷贝操作。但实际上是在引用内存中某个值时隐式走PCIe总线拷贝，这样的方式有几个优点： <ul><li>无需所有数据一次性显式拷贝到设备端，而是<strong>引用某个数据时即时隐式拷贝</strong></li><li>隐式拷贝是<strong>异步</strong>的，可以和计算并行，隐藏内存传输延时</li></ul></li></ul></li><li>On systems with a front-side bus, bandwidth between host memory and device memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#write-combining-memory" target="_blank" rel="noopener noreferrer">Write-Combining Memory<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. <ul><li>对于有前端总线（指连接北桥的总线），使用锁页内存能够取得更高的性能。</li></ul></li></ul><p>也有部分设备，其显存和主机内存物理上是同一块。此时用映射内存更好。可通过相关API查询。</p><blockquote><p><code>cudaHostAlloc</code>的Flag位如下（<a href="https://zhuanlan.zhihu.com/p/188246455" target="_blank" rel="noopener noreferrer">参考<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）：</p><ul><li>cudaHostAllocDefault 默认值，等同于cudaMallocHost。</li><li>cudaHostAllocPortable 分配所有GPU都可使用的锁页内存</li><li>cudaHostAllocMapped。 此标志下分配的锁页内存可实现零拷贝功能，主机端和设备端各维护一个地址，通过地址直接访问该块内存，无需传输。</li><li>cudaHostAllocWriteCombined 将分配的锁页内存声明为write-combined写联合内存，此类内存不使用L1 和L2 cache，所以程序的其它部分就有更多的缓存可用。此外，write-combined内存通过PCIe传输数据时不会被监视，能够获得更高的传输速度。因为没有使用L1、L2cache， 所以主机读取write-combined内存很慢，write-combined适用于主机端写入、设备端读取的锁页内存。</li></ul></blockquote><div class="hint-container info"><p class="hint-container-title">小结</p><p>减少主机内存和显存之间通信的方法：</p><ol><li>让设备干活！让中间数据存在设备上</li><li>许多小传输批量合并为一次大传输</li><li>使用锁页内存、零拷贝内存</li></ol></div><h3 id="设备内存访问" tabindex="-1"><a class="header-anchor" href="#设备内存访问" aria-hidden="true">#</a> 设备内存访问</h3><p>一条访存指令可能会被发射很多很多次！要发射多少次，取决于一个线程束内 各个线程访存地址的分布情况。这种分布方式对指令吞吐量的影响和内存的类型相关，将在下面的章节中进行说明。例如，对于全局存储器，一般来说，<strong>地址越分散，吞吐量就越降低</strong>。</p><h4 id="全局存储器" tabindex="-1"><a class="header-anchor" href="#全局存储器" aria-hidden="true">#</a> 全局存储器</h4><p>全局存储器通过32、64、128<strong>字节</strong>（不是位！）的<strong>内存事务</strong>进行访问。访问时需对齐。</p><p>当一个线程束在访问内存时，会根据其<strong>内部每个线程所需要访存的大小</strong>、<strong>访存地址的分布</strong>对访存进行<strong>合并</strong>，<strong>合并成若干内存事务</strong>。一般来说，需要的事务越多，传输中没有什么用的字也就越多，从而相应地降低了指令吞吐量。例如，如果每个线程的 4 字节访问要产生 32 字节的内存事务，吞吐量就要除以 8。（1次32字节的内存事务，会访问32字节，但其中只有4字节是我需要的）</p><p>需要进行多少次事务处理，以及最终会对吞吐量造成多大影响，都因设备的计算能力而异。（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-5-x" target="_blank" rel="noopener noreferrer">Compute Capability 5.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-6-x" target="_blank" rel="noopener noreferrer">Compute Capability 6.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-7-x" target="_blank" rel="noopener noreferrer">Compute Capability 7.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-8-x" target="_blank" rel="noopener noreferrer">Compute Capability 8.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-9-0" target="_blank" rel="noopener noreferrer">Compute Capability 9.0<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 给出了各种计算能力如何处理全局存储器访问的更多细节。）</p><p>若要<strong>最大化全局存储器的吞吐量</strong>，就要<strong>想方设法尽量去合并访存</strong>：</p><ul><li>根据设备的计算能力，找到最合适的访存模式</li><li>使用符合大小和对齐要求的数据结构</li><li>在一些情况下填充数据</li></ul><h4 id="大小和对齐要求" tabindex="-1"><a class="header-anchor" href="#大小和对齐要求" aria-hidden="true">#</a> 大小和对齐要求</h4><p>可以使用<code>__align__(X)</code>关键字</p><h4 id="二维数组" tabindex="-1"><a class="header-anchor" href="#二维数组" aria-hidden="true">#</a> 二维数组</h4><p>Thread Index为<code>(tx,ty)</code>的线程，若要访问一个宽度为<code>Width</code>的二维数组，访问数组中如下地址较好：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>BaseAddress + width * ty + tx
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><blockquote><p>备注：也是让<code>tx</code>相邻的线程访问相邻的元素，合并同Warp内的内存访问</p></blockquote><p>为了让二维数组中的一行元素的个数为32（线程束中线程个数）的倍数（以进一步合并访存），可以通过<code>cudaMallocPitch()</code> and <code>cuMemAllocPitch()</code>等函数帮我们自动做padding。</p><blockquote><p>Ref：<a href="https://zhuanlan.zhihu.com/p/490239617" target="_blank" rel="noopener noreferrer">CUDA编程：二维数组的分配和使用 - 知乎 (zhihu.com)<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>分配内存</strong></p><p>二维数组实际上也是连续的空间，但是进行了内存对齐。一行的有效宽度为<code>Width</code>，在内存中的实际宽度为<code>Pitch</code>。<em><code>Width</code>和<code>Pitch</code>的单位都是Byte</em> 。</p><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// CPU分配--使用一维数组</span>
<span class="token keyword">float</span> <span class="token operator">*</span>N_h <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token keyword">float</span> <span class="token punctuation">[</span>Width <span class="token operator">*</span> Height<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment">//pitch == width</span>
<span class="token comment">// GPU内存分配</span>
<span class="token keyword">float</span> <span class="token operator">*</span>N_d<span class="token punctuation">;</span> 
size_t Pitch<span class="token punctuation">;</span>
<span class="token function">cudaMallocPitch</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>N_d<span class="token punctuation">,</span> <span class="token operator">&amp;</span>Pitch<span class="token punctuation">,</span> Width<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token punctuation">)</span><span class="token punctuation">,</span> Height<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>初始化</strong></p><p>初始化需要将数组从CPU拷贝上GPU，使用<code>cudaMemcpy2D()</code>函数。函数原型为</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>__host__cudaError_t cudaMemcpy2D (void *dst, size_t dpitch, const void *src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>它将一个Host（CPU）上的二维数组，拷贝到Device（GPU）上。CPU和GPU上的二维数组，都<em><strong>存储成C语言一维数组形式。</strong></em></p><p>参数分别表示</p><ul><li>dst：Device上的数组指针</li><li>dpitch：Device上二维数组的Pitch（一行在内存中的实际宽度）。<strong>单位Byte</strong></li><li>src：Host上的数组指针</li><li>spitch：Host上二维数组的Pitch（一行在内存中的实际宽度）。<strong>单位Byte</strong></li><li>width：二维数组一行的宽度（逻辑上的宽度）。<strong>单位Byte</strong></li><li>height：二维数组的 <em>行数。</em></li><li>kind：数据迁移的类别，如 <code>cudaMemcpyHostToDevice</code></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// 将数据从CPU迁移到GPU</span>
<span class="token function">cudaMemcpy2D</span><span class="token punctuation">(</span>N_d<span class="token punctuation">,</span> Pitch<span class="token punctuation">,</span> N_h<span class="token punctuation">,</span> Width<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token punctuation">)</span><span class="token punctuation">,</span> Width<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token punctuation">)</span><span class="token punctuation">,</span> Height<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>访问</strong></p><p><em>CUDA C Programming Guide</em> 中给出的访问方式如下所示</p><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// 二维数组的访问</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>Height<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>row <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">char</span><span class="token operator">*</span><span class="token punctuation">)</span>N<span class="token operator">+</span>i<span class="token operator">*</span>Pitch<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>Width<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
        row<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">++</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>Pitch</code> 是一行所占的字节数， 先将指针<code>N</code> 强制转化为<code>char*</code>（char 占1Byte，float占3Byte）， 在向后移动<code>Pitch</code>个字节，得到<code>(char*)N+1*Pitch</code> ，它是第1行（从0计数）的首地址；再将它转换回<code>float*</code>，就可以通过这个指针（<code>row</code>） 来访问第1行。</p><p>也正因为<code>Pitch</code>是以字节计数的，所以以下这种索引方式是错误的</p><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">/* 不可以使用以下语句 */</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>Height<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>Width<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
        N<span class="token punctuation">[</span>i<span class="token operator">*</span>N_pitch_d<span class="token operator">+</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>正确写法应该是：</p><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>Height<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>j<span class="token operator">&lt;</span>Width<span class="token punctuation">;</span>j<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
        N<span class="token punctuation">[</span>i<span class="token operator">*</span>N_pitch_d<span class="token operator">/</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">+</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></blockquote><h4 id="局部存储器" tabindex="-1"><a class="header-anchor" href="#局部存储器" aria-hidden="true">#</a> 局部存储器</h4><p>虽然编译器一般会将会将 不使用标识符（如<code>__shared__</code>等）声明出的变量 分配到寄存器中，但是也有少数情况会放在局部存储器（Local Memory）中：</p><ul><li>==无法确定以常量为索引的数组?==没看懂啥意思、 <ul><li>Arrays for which it cannot determine that they are indexed with constant quantities</li></ul></li><li>会<strong>占用过多</strong>寄存器空间的大型结构或数组、</li><li>内核使用的寄存器数目<strong>超过了可用</strong>的寄存器数目（这也被称为寄存器溢出）。</li></ul><p>如何确定变量是不是使用了局部存储器？</p><ul><li>查看汇编出来的<code>ptx</code>： <ul><li>以<code>.local</code>助记符声明</li><li>以<code>ld.local</code>、<code>st.local</code>访存</li></ul></li><li>查看最终编译出的<code>cubin</code>： <ul><li>即便<code>ptx</code>没有体现local，有可能在针对设备最终编译时，会决定放入局部存储器</li><li><code>cuobjdump</code></li></ul></li><li><code>--ptxas-options=-v</code>选项</li></ul><h4 id="共享存储器" tabindex="-1"><a class="header-anchor" href="#共享存储器" aria-hidden="true">#</a> 共享存储器</h4><p>共享存储器被组织为若干Bank。如果一次请求落在了不同的Bank，这些请求可以被同时处理。如果一次请求落在了相同的Bank，这些请求必须依次串行处理。</p><p>所以，很有必要去了解内存地址是如何映射到Bank上的。这和jurisdiction的计算能力有关。（Ref：<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-5-x" target="_blank" rel="noopener noreferrer">Compute Capability 5.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-6-x" target="_blank" rel="noopener noreferrer">Compute Capability 6.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-7-x" target="_blank" rel="noopener noreferrer">Compute Capability 7.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-8-x" target="_blank" rel="noopener noreferrer">Compute Capability 8.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-9-0" target="_blank" rel="noopener noreferrer">Compute Capability 9.0<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）</p><blockquote><p>备注：在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-5-x" target="_blank" rel="noopener noreferrer">Compute Capability 5.x<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>较为详细地说明了Bank Conflict问题</p></blockquote><h4 id="常量存储器" tabindex="-1"><a class="header-anchor" href="#常量存储器" aria-hidden="true">#</a> 常量存储器</h4><p>常量存储器空间位于设备内存中，并在常量缓存中缓存。</p><p>然后，一个请求会根据初始请求中不同的内存地址被拆分成多个单独的请求，吞吐量的下降系数等于单独请求的数量。</p><p>如果<strong>缓存命中</strong>，则按照<strong>常量缓存的吞吐量</strong>为由此产生的请求提供服务；反之，则按照设备内存的吞吐量提供服务。</p><h4 id="纹理和表面存储器" tabindex="-1"><a class="header-anchor" href="#纹理和表面存储器" aria-hidden="true">#</a> 纹理和表面存储器</h4><p>纹理和曲面内存空间位于设备内存中，并缓存在纹理缓存中，因此<strong>只有在缓存未命中时</strong>，纹理获取或曲面读取才会<strong>花费一次从设备内存读取的代价</strong>，否则只需花费<strong>一次从纹理缓存读取内存的代价</strong>。纹理缓存针对 2D 空间位置进行了优化，因此读取纹理或曲面地址的同一线程束如果在 2D 中相距很近，就能获得最佳性能。此外，纹理缓存是为具有恒定延迟的流式读取而设计的；缓存命中会减少 DRAM 带宽需求，但不会减少读取延迟。</p><p>通过纹理或曲面获取读取设备内存<strong>具有一些优势</strong>，可以使其成为从全局存储器或常量存储器读取设备内存的有利替代方案：</p><ul><li><p>如果内存读取不遵循为获得良好性能而必须遵循的全局或常量存储器读取的访问模式（指<strong>没有合并访存</strong>），那么只要<strong>对纹理存储器或表面存储器的访问具有局部性</strong>，就可以获得更高的带宽；</p></li><li><p><strong>寻址计算由专用单元在内核外执行</strong>；</p></li><li><p>打包数据可在一次操作中广播到不同的变量；</p></li><li><p>8 位和 16 位整数输入数据可选择转换为范围为 [0.0, 1.0] 或 [-1.0, 1.0] 的 32 位浮点数值（参见纹理内存）。</p></li></ul><h2 id="最大化指令吞吐量" tabindex="-1"><a class="header-anchor" href="#最大化指令吞吐量" aria-hidden="true">#</a> 最大化指令吞吐量</h2><p>欲最大化指令吞吐量，我们应该这么做：</p><ul><li><strong>Minimize the use of arithmetic instructions with low throughput;</strong> this includes trading precision for speed when it does not affect the end result, such as <strong>using intrinsic instead of regular functions (intrinsic functions are listed in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#intrinsic-functions" target="_blank" rel="noopener noreferrer">Intrinsic Functions<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>), single-precision instead of double-precision, or flushing denormalized numbers to zero;</strong><ul><li>不要使用吞吐量低的算数指令，可以用： <ul><li>一些内建函数（我也不知道咋翻译好了，感觉就是用特殊计算单元而非常规计算单元的函数）</li><li>单精度而不是双精度</li><li>把<a href="https://zh.wikipedia.org/wiki/IEEE_754#%E9%9D%9E%E8%A7%84%E7%BA%A6%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%B5%AE%E7%82%B9%E6%95%B0" target="_blank" rel="noopener noreferrer">非规格化数<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>（即十分接近0的数）直接干成0（编译参数<code>-ftz=true</code>）</li></ul></li></ul></li><li>Minimize divergent warps caused by control flow instructions as detailed in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#control-flow-instructions" target="_blank" rel="noopener noreferrer">Control Flow Instructions<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><ul><li>同一个Warp内的32个线程尽量不在分支指令处“分叉”</li></ul></li><li>Reduce the number of instructions, for example, by optimizing out synchronization points whenever possible as described in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-instruction" target="_blank" rel="noopener noreferrer">Synchronization Instruction<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> or by using restricted pointers as described in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#restrict" target="_blank" rel="noopener noreferrer"><strong>restrict</strong><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>. <ul><li>减少同步指令</li><li>使用<code>__restrict__</code>指针</li></ul></li></ul><p>可以使用如下的编译标志：</p><ul><li>code compiled with <code>-ftz=true</code> (denormalized numbers are flushed to zero) tends to have higher performance than code compiled with <code>-ftz=false</code>. <ul><li>把<a href="https://zh.wikipedia.org/wiki/IEEE_754#%E9%9D%9E%E8%A7%84%E7%BA%A6%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%B5%AE%E7%82%B9%E6%95%B0" target="_blank" rel="noopener noreferrer">非规格化数<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>（即十分接近0的数）直接干成0</li></ul></li><li>Similarly, code compiled with <code>-prec-div=false</code> (less precise division) tends to have higher performance code than code compiled with <code>-prec-div=true</code>, <ul><li>不使用高精度除法</li></ul></li><li>and code compiled with <code>-prec-sqrt=false</code> (less precise square root) tends to have higher performance than code compiled with <code>-prec-sqrt=true</code>. The nvcc user manual describes these compilation flags in more details. <ul><li>不使用高精度平方根</li></ul></li></ul><p>可使用如下的API：</p><p>单精度浮点除法：<code>__fdividef(x, y)</code></p><p>单精度浮点倒数平方根： optimize <code>1.0/sqrtf()</code> into <code>rsqrtf()</code></p><p>单精度浮点平方根：（没琢磨透啥意思）Single-precision floating-point square root is implemented as a reciprocal square root followed by a reciprocal instead of a reciprocal square root followed by a multiplication so that it gives correct results for 0 and infinity.</p><p><strong>三角函数</strong>（<code>sin</code>、<code>cos</code>等）<code>sinf(x)</code>, <code>cosf(x)</code>, <code>tanf(x)</code>, <code>sincosf(x)</code>, and corresponding double-precision instructions开销更大，尤其是当<code>x</code>非常大时。<code>the argument reduction code</code>提供了两条路：一条快路和一条慢路。快速路径用于数量级足够小的参数，主要包括一些乘加运算。慢运算路径用于量级较大的参数，包括在整个参数范围内获得正确结果所需的冗长计算。目前，三角函数的<code>the argument reduction code</code>会为幅度小于 <code>105615.0f </code>的参数（单精度函数）和小于 <code>2147483648.0</code> 的参数（双精度函数）选择快速路径。由于慢速路径比快速路径需要更多的寄存器，因此尝试将一些中间变量存储在本地内存中，以减少慢速路径中的寄存器压力，但由于本地内存的高延迟和带宽可能会影响性能（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses" target="_blank" rel="noopener noreferrer">设备内存访问<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>）【更多请参考原文】。</p><p><strong>整数运算</strong>：<strong>整数除法和取模的开销非常大</strong>！尽量<strong>使用位运算代替</strong>！（例如：If <code>n</code> is a power of 2, (<code>i/n</code>) is equivalent to <code>(i&gt;&gt;log2(n))</code> and <code>(i%n)</code> is equivalent to (<code>i&amp;(n-1)</code>); the compiler will perform these conversions if <code>n</code> is literal.）</p><p><strong>半精度（Half Precision）运算</strong>：大体思想是：<strong>可以把两个16位拼成32位</strong>（<code>half2</code> datatype is used for <code>half</code> precision and <code>__nv_bfloat162</code> be used for <code>__nv_bfloat16</code> precision），<strong>然后调用一次向量函数完成两个16位的运算</strong>（Vector intrinsics (for example, <code>__hadd2</code>, <code>__hsub2</code>, <code>__hmul2</code>, <code>__hfma2</code>) can then be used to do two operations in a single instruction.）。同时还提供了一些助手函数，能够把两个半精度的浮点值转换成<code>half2</code>、<code>__nv_bfloat162</code>（<code>__halves2half2</code>、<code>__halves2bfloat162</code>）</p><p><strong>类型转换</strong>：类型转换会<strong>引入额外的指令、额外的时钟周期</strong>（，所以要<strong>尽量避免类型转换</strong>）。啥时候会发生类型转换呢？</p><ul><li>对<code>char</code>或<code>short</code>类型做运算：实际上会先把他们转换成<code>int</code>再运算</li><li>双精度常量被用于做单精度运算时，会将双精度常量再转为单精度。我们可以在**小数后面加一个<code>f</code>**将其声明为单精度常量，例如<code>3.1415926f</code></li></ul><div class="hint-container info"><p class="hint-container-title">小结</p><ol><li>使用<code>__restrict__</code>指针</li><li>控制编译标志，不用高精度运算</li><li>使用由SFU运算的系列函数</li><li>多多使用位运算</li><li>合并半精度运算，使用向量运算函数</li><li>不使用<code>char</code>、<code>short</code></li><li>做单精度运算，字面值常量后面带上<code>f</code></li></ol></div></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/jielahou/jielahou-blog/edit/main/docs/read_note/cuda_c_programming_guide/chap_8_Performance_guidelines.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">上次编辑于: </span><!----></div><div class="meta-item contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: jielahou@gmail.com">jielahou</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer"></div><div class="copyright">Copyright © 2024 jielahou</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-ce01a4a7.js" defer></script>
  </body>
</html>
